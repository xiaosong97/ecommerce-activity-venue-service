# 电商活动会场后端服务（C端高并发入口）

## 项目背景

在电商与内容平台中，活动会场页通常是C端流量最集中的入口之一。
该类系统需要在极低延迟目标下承载高并发访问，并同时聚合多个下游系统的数据，对并发模型、超时控制与稳定性治理提出了较高要求。

本项目用于复刻 **电商 C 端活动会场高并发入口** 的典型后端场景，作为个人工程实践与系统设计能力的综合演示。

---

## 项目定位与目标

- **系统定位**
  本系统为电商活动会场的后端入口服务，负责聚合多个下游服务数据并返回活动页所需的信息。
  不承担交易、库存扣减等核心交易职责。
- **核心目标**
  - 在单机 **1000 QPS** 负载下稳定运行
  - 端到端响应时间目标 **≤ 100ms**
  - 通过并行调用、超时控制与降级策略保障系统稳定性
- **非目标**
  - 不实现完整下单 / 支付流程
  - 不追求功能完备性，重点关注系统稳定性与工程取舍

---

## 典型业务场景说明

- **访问场景**

  用户访问活动会场页，请求活动基础信息、商品列表及多维度展示数据。

- **下游依赖**

  单次请求需要并行调用 **5个互不依赖的下游服务**，下游响应时间均在 100ms 以内。

- **系统约束**

  - 机器规格：8 Core / 8 GB
  - 高并发入口场景，对尾延迟敏感
  - 不允许长时间排队

---

## 系统架构与请求链路

```text
Client
⬇️
Activity Venue Service
⬇️ ⬇️ ⬇️ ⬇️ ⬇️ (并行 fan-out)
DownStream A、B、C、D、E
⬇️
结果聚合&降级处理
⬇️
Response
```

---

## 并发与性能设计

### 并发容量推导

- 目标 QPS：1000
- 目标响应时间：100ms

根据Little‘s Law：

在途并发 ≈ QPS x RT ≈ 100

- 单次请求需要并行调用5个下游
- 每个下游的并发需求约为100
- 为应对抖动，设置每个下游并发上限为 **120-150**

---

### 并发控制策略

- 下游调用采用 **并行 fan-out 模型**
- 对每个下游设置独立并发隔离（Bulkhead）
- 不允许长队列，请求在高压情况下快速失败而非排队

---

### 超时与降级策略

- 端到端超时：95ms
- 单个下游调用超时：80ms
- 下游超时或者失败时返回兜底数据，避免单点故障放大
- 允许活动页在非核心字段上存在短暂不一致

---

## 技术选型

- Java
- Spring Boot + WebFlux（非阻塞I/O）
- WebClient （下游并行调用）
- 并发控制 Semaphore/Resilience4j
- 压测工具：wrk/hey

---

## 压测与验证

- 单机1000QPS压测
- 验证指标：
  - p95 / p99 / p99.9 延迟
  - 超时比例
  - 下游异常情况下的系统稳定性

---

## 可演进方向

- 多机部署与水平扩展
- 更细力度的降级策略
- 热点活动识别与流量调度
- 监控与告警体系完善

## 说明

本项目为个人在研项目，重点用于系统设计与工程实践能力的展示，不涉及真实业务数据或生产环境部署。



## 系统边界说明

本系统定位为电商活动会场的后端入口服务，主要职责是聚合多个下游系统的数据并返回活动页所需信息。

本系统不承担以下责任：

- 订单创建与支付
- 库存扣减与一致性保障
- 用户鉴权和风控

以上能力由下游系统或其他专用服务负责

